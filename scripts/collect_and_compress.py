# scripts/collect_and_compress.py (å¸¦æ•°æ®è´¨æ£€åŠŸèƒ½)

import pandas as pd
import glob
import os
from tqdm import tqdm
import shutil
import json

# --- é…ç½® ---
INPUT_BASE_DIR = "all_data"
OUTPUT_DIR = "kdata"
FINAL_PARQUET_FILE = "full_kdata.parquet"
QC_REPORT_FILE = "data_quality_report.json" # è´¨æ£€æŠ¥å‘Šæ–‡ä»¶å

def run_quality_check(df):
    """
    å¯¹åˆå¹¶åçš„ DataFrame è¿›è¡Œæ•°æ®è´¨é‡æ£€æŸ¥ï¼Œå¹¶ç”ŸæˆæŠ¥å‘Šã€‚
    """
    print("\n" + "="*50)
    print("ğŸ” å¼€å§‹è¿›è¡Œæ•°æ®è´¨é‡æ£€æŸ¥ (Data Quality Check)...")
    
    report = {}
    
    # 1. åŸºç¡€ç»Ÿè®¡
    report['total_records'] = len(df)
    report['total_stocks'] = df['code'].nunique()
    report['start_date'] = df['date'].min().strftime('%Y-%m-%d')
    report['end_date'] = df['date'].max().strftime('%Y-%m-%d')
    
    # 2. å®Œæ•´æ€§æ£€æŸ¥
    # æ£€æŸ¥æ—¥æœŸæ˜¯å¦è¿ç»­ (æŠ½æ ·æ£€æŸ¥ä¸€åªé•¿å†å²è‚¡ç¥¨)
    long_history_stock = df.groupby('code').size().idxmax()
    df_single = df[df['code'] == long_history_stock].set_index('date').sort_index()
    missing_dates = pd.date_range(start=df_single.index.min(), end=df_single.index.max(), freq='B').difference(df_single.index)
    report['completeness_check'] = {
        'sample_stock': long_history_stock,
        'business_days_missing': len(missing_dates)
    }

    # 3. å‡†ç¡®æ€§æ£€æŸ¥ (å¼‚å¸¸å€¼)
    report['accuracy_checks'] = {
        'negative_prices': df[(df['open'] < 0) | (df['high'] < 0) | (df['low'] < 0) | (df['close'] < 0)].shape[0],
        'zero_prices': df[df['close'] <= 0].shape[0],
        'high_lower_than_low': df[df['high'] < df['low']].shape[0],
        'negative_volume': df[df['volume'] < 0].shape[0]
    }

    # 4. ç©ºå€¼æ£€æŸ¥
    nan_counts = df.isnull().sum()
    report['nan_values'] = nan_counts[nan_counts > 0].to_dict()

    # 5. åˆ†å¸ƒç»Ÿè®¡
    stock_lengths = df.groupby('code').size()
    report['distribution_stats'] = {
        'avg_records_per_stock': round(stock_lengths.mean(), 2),
        'median_records_per_stock': stock_lengths.median(),
        'stocks_over_10_years': (stock_lengths > 250*10).sum(),
        'stocks_over_5_years': (stock_lengths > 250*5).sum(),
        'stocks_under_1_year': (stock_lengths < 250*1).sum()
    }

    print("âœ… æ•°æ®è´¨é‡æ£€æŸ¥å®Œæˆã€‚")
    
    # å°†æŠ¥å‘Šä¿å­˜ä¸º JSON æ–‡ä»¶
    with open(QC_REPORT_FILE, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    print(f"ğŸ“„ è´¨æ£€æŠ¥å‘Šå·²ä¿å­˜åˆ°: {QC_REPORT_FILE}")
    
    # åœ¨æ—¥å¿—ä¸­æ‰“å°ä¸€ä»½ç®€æŠ¥
    print("\n--- æ•°æ®è´¨é‡ç®€æŠ¥ ---")
    print(f"  - æ€»è®°å½•æ•°: {report['total_records']:,}")
    print(f"  - è‚¡ç¥¨æ€»æ•°: {report['total_stocks']}")
    print(f"  - æ•°æ®åŒºé—´: {report['start_date']} to {report['end_date']}")
    print(f"  - å¼‚å¸¸æ•°æ®ç‚¹ (ä»·æ ¼<=0): {report['accuracy_checks']['zero_prices']}")
    print(f"  - å¼‚å¸¸æ•°æ®ç‚¹ (é«˜<ä½): {report['accuracy_checks']['high_lower_than_low']}")
    print(f"  - æ•°æ®è¶…è¿‡10å¹´çš„è‚¡ç¥¨æ•°: {report['distribution_stats']['stocks_over_10_years']}")
    print("----------------------")
    
    # å¦‚æœå‘ç°ä¸¥é‡é—®é¢˜ï¼Œå¯ä»¥è€ƒè™‘è®©è„šæœ¬å¤±è´¥
    if report['accuracy_checks']['zero_prices'] > 0 or report['accuracy_checks']['high_lower_than_low'] > 0:
        print("âš ï¸ è­¦å‘Š: å‘ç°ä¸¥é‡çš„å‡†ç¡®æ€§é—®é¢˜ï¼")
        # exit(1) # å¯ä»¥å–æ¶ˆæ³¨é‡Šï¼Œè®©å·¥ä½œæµåœ¨å‘ç°é—®é¢˜æ—¶å¤±è´¥


def main():
    # ... (æ”¶é›†æ–‡ä»¶çš„éƒ¨åˆ†ä¿æŒä¸å˜) ...
    # ... (åˆå¹¶ã€æ’åºã€å‹ç¼©çš„éƒ¨åˆ†ä¹Ÿä¿æŒä¸å˜) ...
    
    # (å…³é”®) åœ¨æ‰€æœ‰æ–‡ä»¶æ“ä½œå®Œæˆåï¼ŒåŠ è½½æœ€ç»ˆçš„æ’åºå DataFrameï¼Œè¿›è¡Œè´¨æ£€
    # ä¸ºäº†æ•ˆç‡ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨å†…å­˜ä¸­çš„ sorted_df
    # å¦‚æœ sorted_df å­˜åœ¨ä¸”ä¸ä¸ºç©ºï¼Œåˆ™æ‰§è¡Œè´¨æ£€
    # sorted_df = ... 
    
    # ä¸ºäº†è®©é€»è¾‘æ¸…æ™°ï¼Œæˆ‘ä»¬æŠŠä¹‹å‰çš„ä»£ç æ•´åˆè¿›æ¥
    
    # é˜¶æ®µ 1: æ”¶é›†
    if os.path.exists(OUTPUT_DIR): shutil.rmtree(OUTPUT_DIR)
    os.makedirs(OUTPUT_DIR)
    search_pattern = os.path.join(INPUT_BASE_DIR, "**", "*.parquet")
    file_list = glob.glob(search_pattern, recursive=True)
    if not file_list: return
    print(f"ğŸ“¦ å…±æ‰¾åˆ° {len(file_list)} ä¸ªè‚¡ç¥¨æ–‡ä»¶ï¼Œå¼€å§‹æ”¶é›†...")
    for src_path in tqdm(file_list, desc="æ”¶é›†ä¸­"):
        shutil.copy2(src_path, os.path.join(OUTPUT_DIR, os.path.basename(src_path)))
    print(f"âœ… æ–‡ä»¶æ”¶é›†å®Œæˆã€‚")

    # é˜¶æ®µ 2: åˆå¹¶ã€æ’åºã€å‹ç¼©
    print("\nğŸš€ å¼€å§‹åˆ›å»ºåˆå¹¶æ–‡ä»¶...")
    all_parquet_files = glob.glob(os.path.join(OUTPUT_DIR, "*.parquet"))
    if not all_parquet_files: return
    all_dfs = [pd.read_parquet(f) for f in tqdm(all_parquet_files, desc="è¯»å–ä¸­")]
    merged_df = pd.concat(all_dfs, ignore_index=True)
    
    # (ç¡®ä¿ date åˆ—æ˜¯ datetime ç±»å‹ï¼Œä»¥ä¾¿è¿›è¡Œè´¨æ£€)
    merged_df['date'] = pd.to_datetime(merged_df['date'])

    print(f"... æ­£åœ¨æŒ‰è‚¡ç¥¨ä»£ç æ’åº...")
    sorted_df = merged_df.sort_values(by='code', ascending=True).reset_index(drop=True)
    
    print(f"... æ­£åœ¨å†™å…¥æœ€ç»ˆæ–‡ä»¶: {FINAL_PARQUET_FILE} ...")
    sorted_df.to_parquet(FINAL_PARQUET_FILE, index=False, compression='zstd', row_group_size=100000)
    print("âœ… æœ€ç»ˆåˆå¹¶æ–‡ä»¶åˆ›å»ºæˆåŠŸï¼")

    # (å…³é”®) é˜¶æ®µ 3: è¿è¡Œæ•°æ®è´¨é‡æ£€æŸ¥
    run_quality_check(sorted_df)


if __name__ == "__main__":
    main()
