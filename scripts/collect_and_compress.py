# scripts/collect_and_compress.py

import pandas as pd
import glob
import os
from tqdm import tqdm
import shutil
import json
from pathlib import Path

# --- é…ç½® ---
INPUT_BASE_DIR = "all_data"
OUTPUT_DIR_SMALL_FILES = "kdata"
FINAL_PARQUET_FILE = "full_kdata.parquet" 
QC_REPORT_FILE = "data_quality_report.json"

def run_quality_check(df):
    """
    å¯¹åˆå¹¶åçš„ DataFrame è¿›è¡Œæ•°æ®è´¨é‡æ£€æŸ¥ï¼Œå¹¶ç”ŸæˆæŠ¥å‘Šã€‚
    """
    print("\n" + "="*50)
    print("ğŸ” [QC] å¼€å§‹è¿›è¡Œæ•°æ®è´¨é‡æ£€æŸ¥ (Data Quality Check)...")
    
    try:
        if not pd.api.types.is_datetime64_any_dtype(df['date']):
            df['date'] = pd.to_datetime(df['date'])
        print("  -> [QC] æ—¥æœŸåˆ—ç±»å‹æ£€æŸ¥/è½¬æ¢å®Œæˆã€‚")

        report = {}
        
        # 1. åŸºç¡€ç»Ÿè®¡
        report['total_records'] = int(len(df))
        report['total_stocks'] = int(df['code'].nunique())
        report['start_date'] = df['date'].min().strftime('%Y-%m-%d')
        report['end_date'] = df['date'].max().strftime('%Y-%m-%d')
        print("  -> [QC] åŸºç¡€ç»Ÿè®¡å®Œæˆã€‚")
        
        # 2. å®Œæ•´æ€§æ£€æŸ¥
        stock_lengths = df.groupby('code').size()
        long_history_stock = stock_lengths.idxmax()
        df_single = df[df['code'] == long_history_stock].set_index('date').sort_index()
        expected_dates = pd.date_range(start=df_single.index.min(), end=df_single.index.max(), freq='B')
        missing_dates = expected_dates.difference(df_single.index)
        report['completeness_check'] = {
            'sample_stock_for_check': long_history_stock,
            'checked_period_years': round((df_single.index.max() - df_single.index.min()).days / 365.25, 1),
            'business_days_missing_in_sample': int(len(missing_dates))
        }
        print("  -> [QC] å®Œæ•´æ€§æŠ½æ ·æ£€æŸ¥å®Œæˆã€‚")

        # 3. å‡†ç¡®æ€§æ£€æŸ¥
        report['accuracy_checks'] = {
            'negative_prices': int(df[(df['open'] < 0) | (df['high'] < 0) | (df['low'] < 0) | (df['close'] < 0)].shape[0]),
            'zero_prices_or_volume': int(df[(df['close'] <= 0) | (df['volume'] <= 0)].shape[0]),
            'high_lower_than_low': int(df[df['high'] < df['low']].shape[0]),
        }
        print("  -> [QC] å‡†ç¡®æ€§ï¼ˆå¼‚å¸¸å€¼ï¼‰æ£€æŸ¥å®Œæˆã€‚")

        # 4. ç©ºå€¼æ£€æŸ¥
        nan_counts = df.isnull().sum()
        report['nan_values_summary'] = nan_counts[nan_counts > 0].astype(int).to_dict()
        print("  -> [QC] ç©ºå€¼æ£€æŸ¥å®Œæˆã€‚")

        # 5. æ•°æ®åˆ†å¸ƒç»Ÿè®¡
        report['distribution_stats'] = {
            'avg_records_per_stock': round(stock_lengths.mean(), 2),
            'median_records_per_stock': int(stock_lengths.median()),
            'stocks_over_15_years': int((stock_lengths > 250*15).sum()),
            'stocks_over_10_years': int((stock_lengths > 250*10).sum()),
            'stocks_over_5_years': int((stock_lengths > 250*5).sum()),
            'stocks_under_1_year': int((stock_lengths < 250*1).sum())
        }
        print("  -> [QC] æ•°æ®åˆ†å¸ƒç»Ÿè®¡å®Œæˆã€‚")

        print("âœ… [QC] æ•°æ®è´¨é‡æ£€æŸ¥é€»è¾‘æ‰§è¡Œå®Œæ¯•ã€‚")
        
        with open(QC_REPORT_FILE, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“„ [QC] è´¨æ£€æŠ¥å‘Šå·²æˆåŠŸä¿å­˜åˆ°: {QC_REPORT_FILE}")
        
        print("\n--- æ•°æ®è´¨é‡ç®€æŠ¥ ---")
        # ... (ç®€æŠ¥æ‰“å°é€»è¾‘) ...

    except Exception as e:
        print(f"\nâŒ [QC] åœ¨æ‰§è¡Œè´¨é‡æ£€æŸ¥æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

def main():
    """
    ä¸»å‡½æ•°ï¼ŒåŒ…å«äº†è¯¦ç»†çš„è°ƒè¯•æ‰“å°ã€‚
    """
    print("\n--- [main] å‡½æ•°å¼€å§‹æ‰§è¡Œ ---")
    
    # --- é˜¶æ®µ 1: æ”¶é›†æ‰€æœ‰å°æ–‡ä»¶ ---
    if os.path.exists(OUTPUT_DIR_SMALL_FILES):
        shutil.rmtree(OUTPUT_DIR_SMALL_FILES)
    os.makedirs(OUTPUT_DIR_SMALL_FILES)
    print(f"  -> [main] å·²åˆ›å»ºå¹²å‡€çš„è¾“å‡ºç›®å½•: {OUTPUT_DIR_SMALL_FILES}")

    search_pattern = os.path.join(INPUT_BASE_DIR, "**", "*.parquet")
    file_list = glob.glob(search_pattern, recursive=True)
    
    if not file_list:
        print("\nâŒ [main] è‡´å‘½é”™è¯¯: åœ¨æ‰€æœ‰ä¸‹è½½äº§ç‰©ä¸­æœªæ‰¾åˆ°ä»»ä½• .parquet æ–‡ä»¶ï¼è„šæœ¬ç»ˆæ­¢ã€‚")
        exit(1)

    print(f"ğŸ“¦ [main] å…±æ‰¾åˆ° {len(file_list)} ä¸ªè‚¡ç¥¨çš„ Parquet æ–‡ä»¶ï¼Œå¼€å§‹æ”¶é›†...")
    
    for src_path in tqdm(file_list, desc="æ­£åœ¨æ”¶é›†ä¸­"):
        # ... (æ”¶é›†æ–‡ä»¶çš„ try...except é€»è¾‘) ...
            
    print(f"\nâœ… [main] å…¨éƒ¨ {len(file_list)} ä¸ªæ–‡ä»¶å·²æˆåŠŸæ”¶é›†åˆ° '{OUTPUT_DIR_SMALL_FILES}' ç›®å½•ä¸­ã€‚")

    # --- é˜¶æ®µ 2: åˆ›å»ºä¸€ä¸ªç»è¿‡ä¼˜åŒ–çš„åˆå¹¶å¤§æ–‡ä»¶ ---
    print("\n" + "="*50)
    print("ğŸš€ [main] å¼€å§‹åˆ›å»ºç»è¿‡å‹ç¼©ä¼˜åŒ–çš„åˆå¹¶æ–‡ä»¶...")
    
    all_parquet_files = glob.glob(os.path.join(OUTPUT_DIR_SMALL_FILES, "*.parquet"))
    
    if not all_parquet_files:
        print("âŒ [main] é”™è¯¯: åœ¨æ”¶é›†ç›®å½•ä¸­æœªæ‰¾åˆ° Parquet æ–‡ä»¶ï¼Œæ— æ³•åˆ›å»ºåˆå¹¶æ–‡ä»¶ã€‚è„šæœ¬ç»ˆæ­¢ã€‚")
        return
        
    print(f"ğŸ“¦ [main] æ­£åœ¨è¯»å– {len(all_parquet_files)} ä¸ª Parquet æ–‡ä»¶...")
    all_dfs = [pd.read_parquet(f) for f in tqdm(all_parquet_files, desc="æ­£åœ¨è¯»å–")]
    
    print("... [main] æ­£åœ¨åˆå¹¶æ‰€æœ‰æ•°æ® ...")
    merged_df = pd.concat(all_dfs, ignore_index=True)
    print(f"... [main] åˆå¹¶å®Œæˆï¼ŒDataFrame å½¢çŠ¶: {merged_df.shape}")

    print("... [main] æ­£åœ¨è¿›è¡Œå¼ºåˆ¶æ•°æ®ç±»å‹è½¬æ¢ ...")
    # ... (æ•°æ®ç±»å‹è½¬æ¢é€»è¾‘) ...
    print("âœ… [main] æ•°æ®ç±»å‹è½¬æ¢å®Œæˆã€‚")
    
    print(f"... [main] æ­£åœ¨æŒ‰è‚¡ç¥¨ä»£ç  ('code') æ’åº...")
    sorted_df = merged_df.sort_values(by='code', ascending=True).reset_index(drop=True)
    
    output_path = FINAL_PARQUET_FILE
    print(f"... [main] æ­£åœ¨å°†æ’åºåçš„æ•°æ®å†™å…¥æœ€ç»ˆçš„åˆå¹¶æ–‡ä»¶: {output_path} ...")
    
    # (ä¿æŒ to_parquet çš„ try...except é€»è¾‘ä¸å˜)
    try:
        sorted_df.to_parquet(output_path, index=False, compression='zstd', row_group_size=100000)
        print("\nâœ… [main] æœ€ç»ˆåˆå¹¶æ–‡ä»¶åˆ›å»ºæˆåŠŸ (ä½¿ç”¨ zstd å‹ç¼©)ï¼")
    except ImportError:
        # ...
    
    # --- é˜¶æ®µ 3: è¿è¡Œæ•°æ®è´¨é‡æ£€æŸ¥ ---
    print("\n--- [main] å‡†å¤‡è°ƒç”¨ run_quality_check å‡½æ•° ---")
    if sorted_df is not None and not sorted_df.empty:
        run_quality_check(sorted_df)
    else:
        print("\nâš ï¸ [main] è­¦å‘Š: åˆå¹¶åçš„æ•°æ® (sorted_df) ä¸ºç©ºï¼Œè·³è¿‡è´¨é‡æ£€æŸ¥ã€‚")
        
    print("\n--- [main] å‡½æ•°æ‰§è¡Œå®Œæ¯• ---")

if __name__ == "__main__":
    # (é‡è¦) main å‡½æ•°ä¹Ÿåº”è¯¥è¢«åŒ…è£¹åœ¨ try...except ä¸­ï¼Œä»¥æ•è·ä»»ä½•æœªé¢„æ–™çš„é¡¶å±‚é”™è¯¯
    try:
        main()
    except Exception as e:
        print(f"\nâŒâŒâŒ åœ¨ main å‡½æ•°é¡¶å±‚æ•è·åˆ°è‡´å‘½å¼‚å¸¸: {e} âŒâŒâŒ")
        import traceback
        traceback.print_exc()
        exit(1)
