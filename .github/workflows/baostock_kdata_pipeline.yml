# scripts/collect_and_compress.py

import pandas as pd
import glob
import os
from tqdm import tqdm
import shutil
import json
from pathlib import Path# .github/workflows/baostock_kdata_pipeline.yml (æœ€ç»ˆæ­£ç¡®ç‰ˆ)

name: ğŸ‚ Baostock å…¨Aè‚¡æ—¥Kçº¿æ•°æ®åˆ†å¸ƒå¼ä¸‹è½½ (éšæœºè´Ÿè½½å‡è¡¡)

on:
  workflow_dispatch:

jobs:
  prepare:
    name: è·å–åˆ—è¡¨å¹¶éšæœºåŒ–ä»»åŠ¡
    runs-on: ubuntu-latest
    steps:
      - name: ğŸ“¥ Checkout repository
        uses: actions/checkout@v4
      - name: ğŸ Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - name: ğŸ“¦ Install Baostock dependencies
        run: pip install baostock pandas
      - name: ğŸƒ è¿è¡Œè„šæœ¬å¯¹ä»»åŠ¡è¿›è¡Œéšæœºåˆ‡åˆ†
        run: python scripts/prepare_tasks.py
      - name: ğŸ“¤ Upload task slices artifact
        uses: actions/upload-artifact@v4
        with:
          name: task-slices
          path: task_slices/

  download:
    name: ä¸‹è½½åˆ†åŒº (Index ${{ matrix.task_index }})
    needs: prepare
    runs-on: ubuntu-latest
    strategy:
      matrix:
        task_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
      max-parallel: 20
      fail-fast: false
    steps:
      - name: ğŸ“¥ Checkout repository
        uses: actions/checkout@v4
      - name: ğŸ“¥ Download randomized task slices
        uses: actions-hub/download-artifact@v3
        with:
          name: task-slices
          path: tasks/
      - name: ğŸ Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - name: ğŸ“¦ Install dependencies
        run: pip install baostock pandas pyarrow tqdm
      - name: ğŸ“ˆ Run Baostock parallel downloader
        env:
          TASK_INDEX: ${{ matrix.task_index }}
        run: python scripts/download_baostock_parallel.py
      - name: ğŸ“¤ Upload partition artifact
        uses: actions/upload-artifact@v4
        with:
          name: kdata_part_${{ matrix.task_index }}
          path: data_slice/

  collect:
    name: æ”¶é›†ã€æ’åºã€å‹ç¼©å¹¶è´¨æ£€æ‰€æœ‰æ–‡ä»¶
    needs: download
    runs-on: ubuntu-latest
    steps:
      - name: ğŸ“¥ Checkout Repository (for collect script)
        uses: actions/checkout@v4
      - name: ğŸ“¥ Download all data artifacts
        uses: actions/download-artifact@v4
        with:
          path: all_data 
      - name: ğŸ Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - name: ğŸ“¦ Install dependencies for collection
        run: pip install pandas pyarrow tqdm zstandard
      - name: ğŸ—„ï¸ Run script to collect, sort, compress, and quality check
        run: python scripts/collect_and_compress.py
      - name: ğŸ“¤ Upload final dataset (small files)
        uses: actions/upload-artifact@v4
        with:
          name: kdata-small-files
          path: kdata/
      - name: ğŸ“¤ Upload final merged file (large file)
        uses: actions/upload-artifact@v4
        with:
          name: full-kdata-parquet-optimized
          path: full_kdata.parquet
      - name: ğŸ“¤ Upload Data Quality Report
        uses: actions/upload-artifact@v4
        with:
          name: data-quality-report
          path: data_quality_report.json

# --- é…ç½® ---
INPUT_BASE_DIR = "all_data"
OUTPUT_DIR_SMALL_FILES = "kdata"
FINAL_PARQUET_FILE = "full_kdata.parquet" 
QC_REPORT_FILE = "data_quality_report.json"

def run_quality_check(df):
    print("\n" + "="*50)
    print("ğŸ” [QC] å¼€å§‹è¿›è¡Œæ•°æ®è´¨é‡æ£€æŸ¥ (Data Quality Check)...")
    
    try:
        if not pd.api.types.is_datetime64_any_dtype(df['date']):
            df['date'] = pd.to_datetime(df['date'])
        print("  -> [QC] æ—¥æœŸåˆ—ç±»å‹æ£€æŸ¥/è½¬æ¢å®Œæˆã€‚")

        # --- (è¿™æ˜¯å…³é”®) ---
        # ç¡®ä¿ report å­—å…¸è¢«å®Œæ•´åœ°å¡«å……
        report = {}
        
        # 1. åŸºç¡€ç»Ÿè®¡
        report['total_records'] = int(len(df))
        report['total_stocks'] = int(df['code'].nunique())
        report['start_date'] = df['date'].min().strftime('%Y-%m-%d')
        report['end_date'] = df['date'].max().strftime('%Y-%m-%d')
        print("  -> [QC] åŸºç¡€ç»Ÿè®¡å®Œæˆã€‚")
        
        # 2. å®Œæ•´æ€§æ£€æŸ¥
        stock_lengths = df.groupby('code').size()
        long_history_stock = stock_lengths.idxmax()
        df_single = df[df['code'] == long_history_stock].set_index('date').sort_index()
        expected_dates = pd.date_range(start=df_single.index.min(), end=df_single.index.max(), freq='B')
        missing_dates = expected_dates.difference(df_single.index)
        report['completeness_check'] = {
            'sample_stock_for_check': long_history_stock,
            'checked_period_years': round((df_single.index.max() - df_single.index.min()).days / 365.25, 1),
            'business_days_missing_in_sample': int(len(missing_dates))
        }
        print("  -> [QC] å®Œæ•´æ€§æŠ½æ ·æ£€æŸ¥å®Œæˆã€‚")

        # 3. å‡†ç¡®æ€§æ£€æŸ¥
        report['accuracy_checks'] = {
            'negative_prices': int(df[(df['open'] < 0) | (df['high'] < 0) | (df['low'] < 0) | (df['close'] < 0)].shape[0]),
            'zero_prices_or_volume': int(df[(df['close'] <= 0) | (df['volume'] <= 0)].shape[0]),
            'high_lower_than_low': int(df[df['high'] < df['low']].shape[0]),
        }
        print("  -> [QC] å‡†ç¡®æ€§ï¼ˆå¼‚å¸¸å€¼ï¼‰æ£€æŸ¥å®Œæˆã€‚")

        # 4. ç©ºå€¼æ£€æŸ¥
        nan_counts = df.isnull().sum()
        report['nan_values_summary'] = nan_counts[nan_counts > 0].astype(int).to_dict()
        print("  -> [QC] ç©ºå€¼æ£€æŸ¥å®Œæˆã€‚")

        # 5. æ•°æ®åˆ†å¸ƒç»Ÿè®¡
        report['distribution_stats'] = {
            'avg_records_per_stock': round(stock_lengths.mean(), 2),
            'median_records_per_stock': int(stock_lengths.median()),
            'stocks_over_15_years': int((stock_lengths > 250*15).sum()),
            'stocks_over_10_years': int((stock_lengths > 250*10).sum()),
            'stocks_over_5_years': int((stock_lengths > 250*5).sum()),
            'stocks_under_1_year': int((stock_lengths < 250*1).sum())
        }
        print("  -> [QC] æ•°æ®åˆ†å¸ƒç»Ÿè®¡å®Œæˆã€‚")
        # -------------------

        print("âœ… [QC] æ•°æ®è´¨é‡æ£€æŸ¥é€»è¾‘æ‰§è¡Œå®Œæ¯•ã€‚")
        
        with open(QC_REPORT_FILE, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“„ [QC] è´¨æ£€æŠ¥å‘Šå·²æˆåŠŸä¿å­˜åˆ°: {QC_REPORT_FILE}")
        
        # æ‰“å°ç®€æŠ¥
        print("\n--- æ•°æ®è´¨é‡ç®€æŠ¥ ---")
        print(f"  - è‚¡ç¥¨æ€»æ•°: {report.get('total_stocks', 'N/A')}")
        print(f"  - æ€»è®°å½•æ•°: {report.get('total_records', 'N/A'):,}")
        print(f"  - æ•°æ®åŒºé—´: {report.get('start_date', 'N/A')} to {report.get('end_date', 'N/A')}")
        accuracy = report.get('accuracy_checks', {})
        print(f"  - å¼‚å¸¸æ•°æ®ç‚¹ (ä»·æ ¼/æˆäº¤é‡<=0): {accuracy.get('zero_prices_or_volume', 'N/A')}")
        distribution = report.get('distribution_stats', {})
        print(f"  - æ•°æ®è¶…è¿‡10å¹´çš„è‚¡ç¥¨æ•°: {distribution.get('stocks_over_10_years', 'N/A')}")
        print("----------------------")

    except Exception as e:
        print(f"\nâŒ [QC] åœ¨æ‰§è¡Œè´¨é‡æ£€æŸ¥æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def main():
    # ... main å‡½æ•°çš„å…¶ä½™éƒ¨åˆ†ä¸æ‚¨ä¹‹å‰æˆåŠŸè¿è¡Œçš„ç‰ˆæœ¬å®Œå…¨ç›¸åŒ ...
    # ... è¯·ç¡®ä¿è¿™éƒ¨åˆ†ä»£ç æ˜¯å®Œæ•´çš„ ...
    pass # æ›¿æ¢ä¸ºå®Œæ•´çš„ main å‡½æ•°é€»è¾‘

if __name__ == "__main__":
    # (é‡è¦) main å‡½æ•°çš„è°ƒç”¨ä¿æŒä¸å˜
    try:
        main()
    except Exception as e:
        print(f"\nâŒâŒâŒ åœ¨ main å‡½æ•°é¡¶å±‚æ•è·åˆ°è‡´å‘½å¼‚å¸¸: {e} âŒâŒâŒ")
        import traceback
        traceback.print_exc()
        exit(1)
